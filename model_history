model_: Associates with tokenizer_:
    > Dataset vocabulary = 993071 (current Dataset)
    > Input length = 100
    > Stopped during Earlystopping
    > Trained on 
        * epochs = 100
        * batch_size = 512
        * callbacks = Earlystopping and tensorboard(might not available now)
    > epoch history
        * epoch 1
            $ loss -> 0.1291, accuracy -> 0.9786, val_loss -> 0.0158, val_accuracy -> 0.9963
        * epoch 2
            $ loss -> 0.0122, accuracy -> 0.9973, val_loss -> 0.0120, val_accuracy -> 0.9970
        * epoch 3
            $ loss -> 0.0085, accuracy -> 0.9982, val_loss -> 0.0117, val_accuracy -> 0.9969
        * epoch 4
            $ loss -> 0.0067, accuracy -> 0.9986, val_loss -> 0.0113, val_accuracy -> 0.9971
        * epoch 5
            $ loss -> 0.0055, accuracy -> 0.9988, val_loss -> 0.0115, val_accuracy -> 0.9972
